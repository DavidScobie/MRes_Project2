{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import mapvbvd\n",
    "from bart import bart\n",
    "from matplotlib import pyplot as plt\n",
    "import cfl\n",
    "import PlotUtils\n",
    "import scipy.io as sio"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "\"\"\"\n",
    "Load Data \n",
    "\"\"\"\n",
    "# Read file.\n",
    "\n",
    "fullfile='/media/sf_ML_work/BART/rawData_exercise/meas_MID00573_FID48984_rest_stack.dat'\n",
    "\n",
    "twixObj = mapvbvd.mapVBVD(fullfile)\n",
    "sizeObj = len(twixObj)\n",
    "twixObj[sizeObj-1].image.flagRemoveOS = False\n",
    "\n",
    "data_hdr = twixObj[sizeObj-1].hdr\n",
    "\n",
    "noSlices = int(data_hdr.Config.NSlc)\n",
    "matrix = int(data_hdr.Config.ImageColumns)\n",
    "nPhases = int(data_hdr.Config.NPhs)\n",
    "accSpokes = int(data_hdr.Config.RadialViews)\n",
    "\n",
    "del data_hdr\n",
    "print(noSlices)\n",
    "\"12\"\n",
    "print(matrix) \n",
    "\"192\"\n",
    "print(nPhases) \n",
    "\"40\"\n",
    "print(accSpokes) \n",
    "\"13\"\n",
    "\n",
    "\"--------------------------------------\"\n",
    "\"  data_hdr = twixObj[sizeObj-1].hdr \"\n",
    "\"current size 384 26 13 1 12 1 40\"\n",
    "\n",
    "raw_data = np.squeeze(twixObj[sizeObj-1].image[:,:,:,:,:,:,:,:,:,:])\n",
    "print(\"raw_data size = \", raw_data.shape)\n",
    "\" (384, 26, 13, 12, 40) \"\n",
    "\n",
    "\"----------------------------------------------\"\n",
    "\n",
    "if noSlices == 1 :\n",
    "    raw_data = np.permute(raw_data, [0, 1, 2, 4, 3])\n",
    "\n",
    "nCoils = len(raw_data[1])\n",
    "print(\" nCoils = \", nCoils)\n",
    "\n",
    "\"\"\"\n",
    "% raw_data is currently stored in the format of:\n",
    "%    matrix*2 (due to 2x OverSampling in readout directiom);\n",
    "%    nCoils\n",
    "%    accSpokes\n",
    "%    noSlices\n",
    "%    nPhases\n",
    "\n",
    "% so in this case [384, 26, 13, 12, 40]\n",
    "\n",
    "% reorder here just so that i can do gridding per 2D slice on each coil\n",
    "% seperately\n",
    "%data_image is now to be stored in the format of:\n",
    "%    matrix*2 (due to 2x OverSampling in readout directiom);\n",
    "%    accSpokes\n",
    "%    nPhases\n",
    "%    nCoils\n",
    "%    noSlices\n",
    "\n",
    "% so in this case [384, 13, 40, 26, 12]\n",
    "\"\"\"\n",
    "raw_data = np.transpose(raw_data, [0, 2, 4, 1, 3])\n",
    "print(\"raw_data = \", raw_data.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pymapVBVD version 0.4.2\n",
      "Software version: VD\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Scan 3/3, read all mdhs: 578MB [00:02, 217MB/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12\n",
      "192\n",
      "40\n",
      "13\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "raw_data size =  (384, 26, 13, 12, 40)\n",
      " nCoils =  26\n",
      "raw_data =  (384, 13, 40, 26, 12)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "\"now calculate the trajectory - get the radial angles from the raw data file\"\n",
    "\"------------------------------------------------------\"\n",
    "\n",
    "uint16Angle0 = np.uint16(twixObj[sizeObj-1].image.iceParam[:,4])  #why are there 4 angles?\n",
    "uint16Angle1 = np.uint16(twixObj[sizeObj-1].image.iceParam[:,5])\n",
    "uint16Angle2 = np.uint16(twixObj[sizeObj-1].image.iceParam[:,6]) \n",
    "uint16Angle3 = np.uint16(twixObj[sizeObj-1].image.iceParam[:,7]) \n",
    "\n",
    "del twixObj\n",
    "del sizeObj\n",
    "\n",
    "tt1=np.stack((uint16Angle0, uint16Angle1, uint16Angle2, uint16Angle3)) #the 4 angles get stacked together\n",
    "print(\"\\n len(uint16Angle0) = \", len(uint16Angle0), \" tt1= \", tt1.shape)\n",
    "\n",
    "del uint16Angle0\n",
    "del uint16Angle1\n",
    "del uint16Angle2\n",
    "del uint16Angle3\n",
    "\n",
    "radialAngles = []\n",
    "for i in range(accSpokes*nPhases) : #range 520\n",
    "    tt4 = np.array(tt1[:, i], dtype=np.uint16)\n",
    "    radialAngles.append(tt4.view(np.double)) #radial angles is what we make from the angles of the .dat file. shape (520, 1). it is a list\n",
    "print('radial angles',np.shape(radialAngles))\n",
    "del tt1\n",
    "del tt4\n",
    "del i #the temporary ones get deleted\n",
    "\"\"\" \n",
    "  print(\"ii = \", i, \" , \", tt4, \" , \", combinedAngle[i]) \n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\" now create trajectory as used in the sequence, to match the above data \"\n",
    "    \n",
    "npoints = accSpokes * (matrix*2) * nPhases; # (13x192x2x40) = 199680, why do we multiply this by 13?\n",
    "dimensions  = 3\n",
    "\n",
    "trajectory = np.zeros((dimensions, matrix*2, accSpokes, nPhases)) #(3,384,13,40) this is number of points\n",
    "\" weights    = np.zeros((matrix*2, accSpokes, nPhases))\"\n",
    "\n",
    "for phs in range(nPhases) : #40\n",
    "    for lin in range(accSpokes) : #13 \n",
    "        ang = radialAngles[phs*accSpokes + lin] #(0:39 x 13) + (0:12), goes up to 519\n",
    "\n",
    "        cos_angle = math.cos(ang)\n",
    "        sin_angle = math.sin(ang)\n",
    "             \n",
    "        for col in range(matrix*2) : #384\n",
    "            kx = (col - matrix) /2  #(0:383 - 192) /2 because of the 2x oversampling\n",
    "                \n",
    "            trajectory[0, col, lin, phs]=(cos_angle*kx) #fill kx up with angles. (-192:192) \n",
    "            trajectory[1, col, lin, phs]=(sin_angle*kx) #(-192:192) \n",
    "            trajectory[2, col, lin, phs]=0.0 #kz = 0\n",
    "            \"\"\"            \n",
    "            if kx == 0.0 :\n",
    "                weights[col, lin, phs]= 0.25 \n",
    "            else :\n",
    "                weights[col, lin, phs]=  abs(kx) \n",
    "            \"\"\"\n",
    "del sin_angle\n",
    "del cos_angle\n",
    "del col \n",
    "del phs\n",
    "del lin  \n",
    "del kx       "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "                                               "
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " len(uint16Angle0) =  6240  tt1=  (4, 6240)\n",
      "radial angles (520, 1)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "GRIDresult = np.zeros((matrix, matrix, nPhases, noSlices)) # (192,192,40,12)\n",
    "\n",
    "# noSlices = 1\n",
    "for sl in range(noSlices) : #range 12\n",
    "    sliceData = raw_data[:,:,:,:,sl] #[384, 13, 40, 26, 1] , why do we have the 13 in here? \n",
    "\n",
    "    print(\"sliceData \", sliceData.shape)\n",
    "    dataCoilSensitivities = np.zeros((matrix*2, accSpokes*nPhases, 1, nCoils), dtype=complex) #(384,520,1,26) each of the k space points has its own sensitivity!\n",
    "    trajScale = np.zeros((dimensions, matrix*2, accSpokes*nPhases)) #(3,384,520)\n",
    "\n",
    "    \" calculate coil sensitivities \"\n",
    "    \" this stacks up the data over all time points, as we do a temporal average to get a fully sampled equivalent image to calculate the CS from\"\n",
    "    # nPhases = 1 #REMOVE THIS LATER\n",
    "    for ph in range(nPhases) : #40\n",
    "        dataCoilSensitivities[:,ph*accSpokes:(ph+1)*accSpokes, 0, :] = np.squeeze(sliceData[:,:,ph,:]) #remove the [...,1] dimension from the end of sliceData. \n",
    "        # we are just reshaping here effectively\n",
    "        trajScale[:,:,ph*accSpokes:(ph+1)*accSpokes] = trajectory[:,:,:, ph] #saving the trajectory of each slice as 3D matrix instead of 4D\n",
    "\n",
    "    dataCS_3124 = np.transpose(dataCoilSensitivities, [2, 0, 1, 3]) #permute and save raw data as another variable before deletion\n",
    "    del dataCoilSensitivities\n",
    "    del ph\n",
    "\n",
    "    \"grid the data\"\n",
    "    \"David: you need to chnage the following line to your gridder!!!!\"\n",
    "    print('trajScale',np.shape(trajScale),'dataCS_3124',np.shape(dataCS_3124))\n",
    "\n",
    "    import tensorflow_nufft as tfft\n",
    "    import tensorflow as tf\n",
    "    # import tensorflow_mri as tfmr\n",
    "    \n",
    "    #reshape the raw data\n",
    "    dataCS_perm = tf.transpose(dataCS_3124, perm=[3,0,1,2])\n",
    "    print('look at this shape', tf.shape(dataCS_perm) )\n",
    "    dataCS_perm =  tf.reshape(dataCS_perm , [nCoils, -1])\n",
    "    print('dataCS_perm',tf.shape(dataCS_perm))\n",
    "\n",
    "    #reshape the trajectory data\n",
    "    trajSc = trajScale[0:2,:,:] #making it (2,384,520)\n",
    "    print('traj_scale',tf.shape(trajSc))\n",
    "    trajSc= tf.transpose(trajSc, perm=[1,2,0])\n",
    "    trajSc =  tf.reshape(trajSc , [-1 , 2])\n",
    "    trajSc = tf.expand_dims(trajSc,axis=0)\n",
    "    trajSc = tf.repeat(trajSc, repeats = 26, axis = 0)\n",
    "    # weights = tfmr.estimate_density(trajSc, (192,192)) #REMOVE THIS\n",
    "\n",
    "    #scale from -pi to pi\n",
    "    trajSc = (trajSc / 192) *2*np.pi\n",
    "    print('trajSc',tf.shape(trajSc), 'max traj', np.max(trajSc), 'min traj', np.min(trajSc))\n",
    "\n",
    "    print('TYPES', 'dataCS_perm', dataCS_perm.dtype, 'trajSc', trajSc.dtype)\n",
    "    average_gridded_data = tfft.nufft(dataCS_perm , trajSc, transform_type='type_1', fft_direction='backward', grid_shape=(192,192))\n",
    "    # average_gridded_data = tf.math.reduce_sum(average_gridded_data,axis = 1)\n",
    "    print('average_gridded_data',tf.shape(average_gridded_data), 'max avg', np.max(average_gridded_data), 'min avg', np.min(average_gridded_data))\n",
    "    average_gridded_data= tf.transpose(average_gridded_data, perm=[1,2,0]) #big set\n",
    "    print(\"average_gridded_data = \", average_gridded_data.shape)\n",
    "    # average_gridded_data = bart(1,\"nufft -i -d\"+str(matrix)+\":\"+str(matrix)+\":1\", trajScale, dataCS_3124) #The bart nufft\n",
    "    \" size (192x192x1x26)\"\n",
    "\n",
    "    \" this part is fine \"\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=4, figsize=(10,10))\n",
    "    for cl in range(4) :\n",
    "        ax[cl].imshow(abs(average_gridded_data[:,:,cl]))\n",
    "    \"  \"\n",
    "    del dataCS_3124\n",
    "    average_gridded_data = tf.expand_dims(average_gridded_data,axis=2)\n",
    "    print(\"average_gridded_data exp dims = \", average_gridded_data.shape)\n",
    "    ksp = bart(1, \"fft -u 7\", average_gridded_data.numpy()) #The coil sensitivities are found by taking the fourier transform of the sum over the images\n",
    "    del average_gridded_data\n",
    "\n",
    "    coil_sensitivities = bart(1, \"caldir 20\", ksp) #this is a calibration of the sensitivities\n",
    "    \" coil_sensivitoes = np.squeeze(coil_sensitivities) \"\n",
    "    del ksp\n",
    "    \" plot coil sensitivities\"\n",
    "    print(\"sl = \", sl, \" , cS size = \", coil_sensitivities.shape)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=5,figsize=(10,10))\n",
    "    for cl in range(5) :\n",
    "        ax[cl].imshow(abs(coil_sensitivities[:,:,0,cl]))\n",
    "    \" \"\n",
    "    \n",
    "#     \" ---------------------------------------------- \"\n",
    "#     \"now grid the data\"\n",
    "\n",
    "    \" slice data is currently [matrix*2, accSpokes, nCoils, nPhases] \"\n",
    "    import tensorflow_mri as tfmr\n",
    "\n",
    "    noSlices = 1 #loop over the 12 of them later\n",
    "    ab_squeeze = np.zeros((40,192,192)) #for plotting at end\n",
    "    nPhases = 10 #the number of temporal frames looking at\n",
    "    for i in range (nPhases):\n",
    "\n",
    "        dataCoilSensitivities = np.zeros((matrix*2, accSpokes, 1, nCoils), dtype=complex) #(384,13,1,26) \n",
    "        trajScale = np.zeros((dimensions, matrix*2, accSpokes)) #(3,384,13)\n",
    "\n",
    "        dataCoilSensitivities[:,:, 0, :] = np.squeeze(sliceData[:,:,i,:])\n",
    "        trajScale[:,:,:] = trajectory[:,:,:, i] \n",
    "\n",
    "        trajSc = trajScale[0:2,:,:] \n",
    "        trajSc= tf.transpose(trajSc, perm=[1,2,0])\n",
    "        trajSc =  tf.reshape(trajSc , [-1 , 2])\n",
    "        trajSc = tf.expand_dims(trajSc,axis=0)\n",
    "        trajSc = tf.repeat(trajSc, repeats = 26, axis = 0)\n",
    "        trajSc = (trajSc / 192) *2*np.pi #scale between -pi and pi\n",
    "        print('trajSc',tf.shape(trajSc), 'max traj', np.max(trajSc), 'min traj', np.min(trajSc)) #trajSc (26,4992,2) here\n",
    "        weights = tfmr.estimate_density(trajSc, (192,192))\n",
    "\n",
    "        # print('data_coil sensitivities', np.shape(dataCoilSensitivities), 'trajScale', np.shape(trajScale))\n",
    "\n",
    "        first_time_data = tf.transpose(dataCoilSensitivities, perm=[2,0,1,3]) #(26,13,384)\n",
    "        dataCS_perm = tf.transpose(first_time_data, perm=[3,0,1,2])\n",
    "        first_time_data = tf.reshape( dataCS_perm, [26 , -1]) #(26,4992) to enter to nufft yes\n",
    "        \n",
    "        # print('first_time_data', tf.shape(first_time_data), 'trajSc', tf.shape(trajSc),'trajSc_type',trajSc.dtype) \n",
    "        ave_gridded_data = tfft.nufft(tf.cast(first_time_data,tf.complex128) , tf.cast(trajSc,tf.float64), transform_type='type_1', fft_direction='backward', grid_shape=(192,192))\n",
    "        # print('ave_gridded_data', tf.shape(ave_gridded_data))\n",
    "\n",
    "        # print('i = ', i)\n",
    "        plt.figure(i+3)\n",
    "        ab_squeeze[i,:,:] = abs(np.squeeze(ave_gridded_data[0,:,:])) #looking at a particular coil\n",
    "        plt.imshow(np.squeeze(ab_squeeze[i,:,:])) \n",
    "\n",
    "    plt.figure(nPhases+1)\n",
    "    plt.imshow(np.sum(ab_squeeze,axis=0))\n",
    "    plt.show()\n",
    "\n",
    "#         \"\"\"   \n",
    "#         print(\"gridded_data2 = \", gridded_data2.shape)\n",
    "#         print(\"coil_sensitivities = \", coil_sensitivities.shape)\n",
    "#         weightedData = abs(np.multiply(gridded_data2, np.conjugate(np.squeeze(coil_sensitivities))))\n",
    "#         print(\"weigghtedData = \", weightedData.shape)\n",
    "     \n",
    "#         tempData = np.sum(weightedData, axis=2) \n",
    "#         print(\"tempData = \", tempData.shape)\n",
    "      \n",
    "#         GRIDresult[:,:,ph,sl] = abs(tempData)\n",
    "#         \"\"\"  \n",
    "#     del ph  \n",
    "#     \"\"\"\n",
    "#     print(\"sl = \", sl  )\n",
    "#     fig, ax = plt.subplots(nrows=5, figsize=(6,10))\n",
    "#     for ph in range(5) :\n",
    "#         ax[ph].imshow(np.squeeze(abs(GRIDresult[:,:,ph,sl])), vmin=0, vmax =0.001)\n",
    "#     \" ---------------------------------------------- \"\n",
    "#     \"\"\"\n",
    "#     del coil_sensitivities\n",
    "    \n",
    "# del dimensions\n",
    "# del radialAngles"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sliceData  (384, 13, 40, 26)\n",
      "trajScale (3, 384, 520) dataCS_3124 (1, 384, 520, 26)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NotFoundError",
     "evalue": "libcudart.so.11.0: cannot open shared object file: No such file or directory",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4788/3317165681.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trajScale'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajScale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dataCS_3124'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataCS_3124\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_nufft\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# import tensorflow_mri as tfmr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_nufft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_nufft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__about__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_nufft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnufft_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_nufft/python/ops/nufft_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m _nufft_ops = tf.load_op_library(\n\u001b[0m\u001b[1;32m     24\u001b[0m   tf.compat.v1.resource_loader.get_path_to_datafile('_nufft_ops.so'))\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\u001b[0m in \u001b[0;36mload_op_library\u001b[0;34m(library_filename)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0munable\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpython\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0mlib_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpy_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_LoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrary_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     wrappers = _pywrap_python_op_gen.GetPythonWrappers(\n",
      "\u001b[0;31mNotFoundError\u001b[0m: libcudart.so.11.0: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}